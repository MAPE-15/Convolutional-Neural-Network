{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "fe1368e4c8a47c02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:06:00.435264Z",
     "start_time": "2024-10-01T20:06:00.432092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "from PIL import Image\n",
    "import mediapipe as mp  # to get facial landmarks from the face images"
   ],
   "id": "db3498eb397da313",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Constants",
   "id": "dfcf95a14f20c9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:06:01.928967Z",
     "start_time": "2024-10-01T20:06:01.925697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data_rgb\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")"
   ],
   "id": "9f7fcc1d59d4d11e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convolutional Neural Network",
   "id": "7edbbe1dbb3989b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reading Data\n",
    "Data are face images.  \n",
    "If image is in grayscale, there is only 1 channel, for RGB image there would be 3 channels.  \n",
    "\n",
    "This section extracts the facial landmarks from the images and labels.\n",
    "\n",
    "1. Read images, from each image extract the facial landmarks, remember the image's label (f.e. happy, sad, ...). We now have fiacl landmarks for each image and label names lists.\n",
    "2. Map the label names list, each label name will now have its own identificator. (f.e. happy = 0, sad = 1, ...). We now have labels list.\n",
    "3. Convert the labels list into a one-hot encoded vector array."
   ],
   "id": "c08143d6f64ad20f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:06:04.381636Z",
     "start_time": "2024-10-01T20:06:04.370116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize MediaPipe Face Mesh model to extract facial landmarks\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)  # static_image_mode=True since we extract from images and not live video\n",
    "\n",
    "def extract_landmarks(image_array: np.array, is_greyscale: bool) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract facial landmarks from the image using MediaPipe.\n",
    "    :param image_array: 2D numpy array representing the image\n",
    "    :param is_greyscale: True if the image is greyscale, False image is in RGB\n",
    "    :return: numpy array containing the normalized facial landmarks\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_greyscale:  # convert image array to RGB format, since MediaPipe works with RGB\n",
    "        image_rgb = np.stack((image_array,) * 3, axis=-1)  # convert grayscale to RGB format\n",
    "    else:\n",
    "        image_rgb = image_array  # image already is RGB\n",
    "        \n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if not results.multi_face_landmarks:\n",
    "        return None  # no face detected, return None\n",
    "\n",
    "    landmarks = results.multi_face_landmarks[0].landmark\n",
    "    image_height, image_width = image_array.shape[:2]\n",
    "    \n",
    "    # normalize the landmarks by the width and height of the image\n",
    "    landmarks_array = np.array([(lm.x * image_width, lm.y * image_height) for lm in landmarks])\n",
    "\n",
    "    return landmarks_array"
   ],
   "id": "7a978775470d8c7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727813164.379600  433392 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n",
      "W0000 00:00:1727813164.381338  510229 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:37:43.262738Z",
     "start_time": "2024-10-01T20:37:43.255440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data(data_dir: str, is_greyscale: bool, image_size=(48, 48)) -> (np.array, np.array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reading data_greyscale from data_dir.\n",
    "    Each image is processed and converted into a numpy array. Then facial landmarks are extrcated from the image array and the facial landmarks are normalized.\n",
    "    \n",
    "    Image is expected to be in grayscale (.convert('L')), and its size is expected to be 48x48 (default).\n",
    "    \n",
    "    Expected data_greyscale directory tree:\n",
    "    - data_dir\n",
    "        - facial_expression_dir1\n",
    "            - image1\n",
    "            - image2\n",
    "            - ...\n",
    "        - facial_expression_dir2 \n",
    "        - facial_expression_dir3\n",
    "        - ...\n",
    "        - facial_expression_dirN\n",
    "        \n",
    "    :param data_dir: directory from which the images are processed\n",
    "    :param image_size: size of the images, default is 48x48\n",
    "    :param is_greyscale: True if the images are greyscale, False images are in RGB\n",
    "    :return: two numpy arrays, first numpy array has stored all the facial landmarks images, and second numpy array has stored all the label names\n",
    "    \"\"\"\n",
    "    \n",
    "    failed_image_landmarks = {}\n",
    "    \n",
    "    image_landmarks = []\n",
    "    label_names = []\n",
    "    \n",
    "    for expression_dirname in sorted(os.listdir(data_dir)):  \n",
    "        # get every directory, this directory contains images of facial expressions \n",
    "        \n",
    "        expression_dir = os.path.join(data_dir, expression_dirname)\n",
    "        \n",
    "        if not os.path.isdir(expression_dir):  # process only directories, skip non-directories - files\n",
    "            continue\n",
    "            \n",
    "        for expression_image in os.listdir(expression_dir):\n",
    "            # get every image in the directory\n",
    "            image_path = os.path.join(expression_dir, expression_image)\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                if is_greyscale:\n",
    "                    image = Image.open(image_path).convert('L')  # L mode, because images are grayscaled\n",
    "                else:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")  # RGB mode, because images are not grayscaled, and therefore they are RGB \n",
    "\n",
    "                image = image.resize(image_size)  # resize the image to expected 48x48\n",
    "                \n",
    "                # convert image into an array, image is a 2D array\n",
    "                image_array = np.array(image)\n",
    "                \n",
    "                # extract normlized facial landmarks\n",
    "                landmarks = extract_landmarks(image_array, is_greyscale)\n",
    "                \n",
    "                if landmarks is not None:\n",
    "                    # successfully the facial landmarks were extracted from the image\n",
    "                    image_landmarks.append(image_array)\n",
    "                    label_names.append(expression_dirname)  # directory name is already a label name of a facial expression\n",
    "                else:\n",
    "                    # failed to get facial landmarks, add to the counter of failed facial landmarks\n",
    "                    \n",
    "                    if expression_dirname not in failed_image_landmarks:\n",
    "                        failed_image_landmarks[expression_dirname] = {\n",
    "                            \"all_images_count\": len(os.listdir(expression_dir)),\n",
    "                            \"failed_images\": [os.path.split(image_path)[-1]]\n",
    "                        }\n",
    "                    else:\n",
    "                        failed_image_landmarks[expression_dirname][\"failed_images\"].append(os.path.split(image_path)[-1])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image: {image_path}\")\n",
    "                print(e)\n",
    "    \n",
    "    if failed_image_landmarks:\n",
    "        print(\"Failed image landmarks: \\n\")\n",
    "        \n",
    "        all_images_count = 0\n",
    "        all_failed_images_count = 0\n",
    "        \n",
    "        for expression, info_dict in failed_image_landmarks.items():\n",
    "            \n",
    "            all_expression_images_count = info_dict[\"all_images_count\"]\n",
    "            images_failed = info_dict[\"failed_images\"]\n",
    "            \n",
    "            n_failed_images = len(images_failed)\n",
    "            fail_ratio = round(n_failed_images / all_expression_images_count * 100, 2)\n",
    "            \n",
    "            print(f\"{expression.upper()} - FAILED: {n_failed_images}/{all_expression_images_count} - {fail_ratio}%\")\n",
    "            for image_failed in images_failed:\n",
    "                print(f\"\\t - {image_failed}\")\n",
    "            print(\"\")\n",
    "            \n",
    "            all_images_count += all_expression_images_count\n",
    "            all_failed_images_count += n_failed_images\n",
    "        \n",
    "        overall_fail_ratio = round(all_failed_images_count / all_images_count * 100, 2)\n",
    "        print(f\"OVERALL - FAILED: {all_failed_images_count}/{all_images_count} - {overall_fail_ratio}%\")\n",
    "            \n",
    "    # convert images and labels list into numpy arrays adn return them\n",
    "    return np.array(image_landmarks), np.array(label_names)            "
   ],
   "id": "625c2057213acb0d",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:39:06.403783Z",
     "start_time": "2024-10-01T20:37:44.876720Z"
    }
   },
   "cell_type": "code",
   "source": "train_image_landmarks, train_label_names = get_data(TRAIN_DIR, is_greyscale=False, image_size=(128, 128))",
   "id": "39db644581ce34dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed image landmarks: \n",
      "\n",
      "ANGRY - FAILED: 11/1175 - 0.94%\n",
      "\t - cropped_emotions.158037~angry.png\n",
      "\t - cropped_emotions.231444~angry.png\n",
      "\t - b389cdd600862ce3acfecaaafed2f444e6d5f4a49249b80d3b3fe458~angry.jpg\n",
      "\t - cropped_emotions.505992~angry.png\n",
      "\t - cropped_emotions.157957~angry.png\n",
      "\t - cropped_emotions.505802~angry.png\n",
      "\t - 6d01a9729bb4b4ad81b039ac13652ce6c03c312c28bbff5672943df4~angry.jpg\n",
      "\t - 0dc59c2bc0d07d830947534205256c306f6982e8ddcde0b39d2d991f~angry.jpg\n",
      "\t - cropped_emotions.231896~angry.png\n",
      "\t - cropped_emotions.231606~angry.png\n",
      "\t - cropped_emotions.506057~angry.png\n",
      "\n",
      "HAPPY - FAILED: 12/2909 - 0.41%\n",
      "\t - cropped_emotions.569571.png\n",
      "\t - cropped_emotions.506066.png\n",
      "\t - cropped_emotions.505964.png\n",
      "\t - cropped_emotions.505782.png\n",
      "\t - cropped_emotions.505768.png\n",
      "\t - 0c0bbcbcf39bd2fa6adf6c99c6cea4bea652d554f0324dcae6b98d2e.jpg\n",
      "\t - 0e19f6d0fea30d22759e1243671c8e3dda4c9ea8c857925f8374445d.jpg\n",
      "\t - cropped_emotions.505759.png\n",
      "\t - cropped_emotions.506097.png\n",
      "\t - cropped_emotions.506135.png\n",
      "\t - cropped_emotions.505985.png\n",
      "\t - 0b2b1189bac832e2edfbfe3b965b1e4985e2852167ff7695e24c0ba2.jpg\n",
      "\n",
      "NEUTRAL - FAILED: 10/3157 - 0.32%\n",
      "\t - cropped_emotions.278424f.png\n",
      "\t - 0e99ee712517d794023494fdeaf602b8d468c8f88f09a0e0a802062df.jpg\n",
      "\t - cropped_emotions.278281f.png\n",
      "\t - 0f21cae9eb345f3bbbd62856317910c2e2ce7c0cfde531fbe9da3769f.jpg\n",
      "\t - cropped_emotions.277579f.png\n",
      "\t - cropped_emotions.278462f.png\n",
      "\t - 0ac2d67161c0f04fbf02003c57bed3420d1e895431d05bdb106914a2f.jpg\n",
      "\t - cropped_emotions.277878f.png\n",
      "\t - cropped_emotions.277589f.png\n",
      "\t - 1a89e4faee1ea6915b6aa8e1dd198fea2705f9a4ec59cfa8086382fbf.jpg\n",
      "\n",
      "SAD - FAILED: 19/3059 - 0.62%\n",
      "\t - cropped_emotions.499092.png\n",
      "\t - cropped_emotions.189102.png\n",
      "\t - cropped_emotions.252225.png\n",
      "\t - cropped_emotions.167620.png\n",
      "\t - 2f24ffcde3faf21f96b4e8f9a6a17769d72e53dce97e5154e69f0f58.jpg\n",
      "\t - cropped_emotions.164628.png\n",
      "\t - cropped_emotions.251951.png\n",
      "\t - 3cb5dac02765c646ea6fbf43f3a8477278609812b9cae44756b4858b.jpg\n",
      "\t - cropped_emotions.251942.png\n",
      "\t - cropped_emotions.5742.png\n",
      "\t - cropped_emotions.151329.png\n",
      "\t - 2a5dbbeec0f075b8164ace2d26ca99b45765faf0cbd2098c865c5c02.jpg\n",
      "\t - 2ea80de8ef9df0e73026c8ad35011cc2ba56dfdbe16c88f67303db35.jpg\n",
      "\t - 1c5423ca78048017254144f19f6351c23f5c20c5c9280ba97dcd1bc7.jpg\n",
      "\t - cropped_emotions.191031.png\n",
      "\t - cropped_emotions.189119.png\n",
      "\t - 03d877a91236cc95bec499ab1bcf6c0b3bb0d9057ae46a75a351f3eb.jpg\n",
      "\t - 6b3cf22bec9b35331e070307b633f032b615eb10230e3c1245b742d4.jpg\n",
      "\t - 8ba074ba747d353038512b10f5f73649020cfb1824b0026db437572a.jpg\n",
      "\n",
      "SURPRISE - FAILED: 3/1109 - 0.27%\n",
      "\t - cropped_emotions.262351~12fffff.png\n",
      "\t - cropped_emotions.264079~12fffff.png\n",
      "\t - cropped_emotions.100087~12fffff.png\n",
      "\n",
      "OVERALL - FAILED: 55/11409 - 0.48%\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:39:30.381477Z",
     "start_time": "2024-10-01T20:39:11.482072Z"
    }
   },
   "cell_type": "code",
   "source": "test_image_landmarks, test_label_names = get_data(TEST_DIR, is_greyscale=False, image_size=(128, 128))",
   "id": "650de7374c5aa3b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed image landmarks: \n",
      "\n",
      "ANGRY - FAILED: 1/131 - 0.76%\n",
      "\t - cropped_emotions.231456~angry.png\n",
      "\n",
      "HAPPY - FAILED: 4/728 - 0.55%\n",
      "\t - 0a53bca982e8866db87a5e28a287f2d611193aa0b6d79afe9027a9ce.jpg\n",
      "\t - cropped_emotions.505787.png\n",
      "\t - cropped_emotions.505810.png\n",
      "\t - 1a08641071f10b7f0760198c6b452594b1c60ce95c948ffcbbff5766.jpg\n",
      "\n",
      "NEUTRAL - FAILED: 3/790 - 0.38%\n",
      "\t - cropped_emotions.278146f.png\n",
      "\t - cropped_emotions.452763f.png\n",
      "\t - 1a4733af85d6fce3a89e6b9d56743f42a538559c3e2bacf0a11bc885f.jpg\n",
      "\n",
      "SAD - FAILED: 4/765 - 0.52%\n",
      "\t - 5f5c6a9881321668025291f83b6849c148e41e9ce3e19187c8d39a66.jpg\n",
      "\t - cropped_emotions.171835.png\n",
      "\t - 6fbc4cef2d9c7b6d5fd1edf10ec888e89ff77b27c9d9474965c40553.jpg\n",
      "\t - 7f07f0d3a27ccba82b024ef149190b12a7df9bb815c2120d07d85336.jpg\n",
      "\n",
      "OVERALL - FAILED: 12/2414 - 0.5%\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:39:49.216989Z",
     "start_time": "2024-10-01T20:39:49.210649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train Image Landmarks:\")\n",
    "print(train_image_landmarks.shape)\n",
    "print(train_image_landmarks, '\\n')\n",
    "\n",
    "print(\"Test Image Landmarks:\")\n",
    "print(test_image_landmarks.shape)\n",
    "print(test_image_landmarks, '\\n')\n",
    "\n",
    "print(\"Train Labels:\")\n",
    "print(train_label_names.shape)\n",
    "print(train_label_names, '\\n')\n",
    "\n",
    "print(\"Test Labels:\")\n",
    "print(test_label_names.shape)\n",
    "print(test_label_names, '\\n')"
   ],
   "id": "c2812cb7cf9c7225",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Landmarks:\n",
      "(11354, 128, 128, 3)\n",
      "[[[[108  13  21]\n",
      "   [109  14  22]\n",
      "   [107  15  20]\n",
      "   ...\n",
      "   [ 75  41  35]\n",
      "   [ 71  34  30]\n",
      "   [ 68  28  26]]\n",
      "\n",
      "  [[121  20  30]\n",
      "   [117  17  25]\n",
      "   [113  15  20]\n",
      "   ...\n",
      "   [ 75  43  35]\n",
      "   [ 73  36  32]\n",
      "   [ 70  32  29]]\n",
      "\n",
      "  [[132  24  34]\n",
      "   [126  19  28]\n",
      "   [119  14  20]\n",
      "   ...\n",
      "   [ 79  46  39]\n",
      "   [ 77  43  39]\n",
      "   [ 76  38  35]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[145 104  80]\n",
      "   [140  99  75]\n",
      "   [129  89  64]\n",
      "   ...\n",
      "   [ 45  27  27]\n",
      "   [ 45  25  24]\n",
      "   [ 44  25  21]]\n",
      "\n",
      "  [[165 120  89]\n",
      "   [158 113  83]\n",
      "   [147 102  72]\n",
      "   ...\n",
      "   [ 44  26  26]\n",
      "   [ 45  25  24]\n",
      "   [ 46  27  23]]\n",
      "\n",
      "  [[177 128  95]\n",
      "   [169 120  87]\n",
      "   [158 113  79]\n",
      "   ...\n",
      "   [ 44  26  26]\n",
      "   [ 45  27  25]\n",
      "   [ 47  28  24]]]\n",
      "\n",
      "\n",
      " [[[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[ 24   2   1]\n",
      "   [ 25   2   1]\n",
      "   [ 24   2   1]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[173 150 149]\n",
      "   [177 150 149]\n",
      "   [177 145 144]\n",
      "   ...\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]]\n",
      "\n",
      "  [[167 148 147]\n",
      "   [172 150 149]\n",
      "   [174 148 147]\n",
      "   ...\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]]\n",
      "\n",
      "  [[163 145 145]\n",
      "   [168 148 147]\n",
      "   [171 150 149]\n",
      "   ...\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]\n",
      "   [  1   1   1]]]\n",
      "\n",
      "\n",
      " [[[135 121  86]\n",
      "   [135 121  86]\n",
      "   [135 121  86]\n",
      "   ...\n",
      "   [151 152 147]\n",
      "   [151 152 147]\n",
      "   [151 152 147]]\n",
      "\n",
      "  [[136 122  87]\n",
      "   [135 121  86]\n",
      "   [135 121  86]\n",
      "   ...\n",
      "   [147 148 143]\n",
      "   [146 147 142]\n",
      "   [146 147 142]]\n",
      "\n",
      "  [[137 123  88]\n",
      "   [135 121  86]\n",
      "   [135 121  86]\n",
      "   ...\n",
      "   [143 144 139]\n",
      "   [141 142 137]\n",
      "   [140 141 136]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[245 244 250]\n",
      "   [243 242 248]\n",
      "   [243 242 245]\n",
      "   ...\n",
      "   [135 112 101]\n",
      "   [107  78  67]\n",
      "   [ 99  71  60]]\n",
      "\n",
      "  [[245 244 250]\n",
      "   [243 242 248]\n",
      "   [242 242 241]\n",
      "   ...\n",
      "   [131 109  97]\n",
      "   [101  72  61]\n",
      "   [102  74  64]]\n",
      "\n",
      "  [[245 244 250]\n",
      "   [243 242 248]\n",
      "   [241 241 241]\n",
      "   ...\n",
      "   [122 100  88]\n",
      "   [ 93  64  53]\n",
      "   [104  77  66]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[239 221 174]\n",
      "   [206 186 144]\n",
      "   [178 160 119]\n",
      "   ...\n",
      "   [ 37  29  37]\n",
      "   [ 34  27  35]\n",
      "   [ 42  35  42]]\n",
      "\n",
      "  [[200 182 135]\n",
      "   [195 176 133]\n",
      "   [126 109  65]\n",
      "   ...\n",
      "   [ 74  56  66]\n",
      "   [ 55  40  47]\n",
      "   [ 65  52  52]]\n",
      "\n",
      "  [[198 183 137]\n",
      "   [197 183 139]\n",
      "   [187 172 130]\n",
      "   ...\n",
      "   [ 99  70  74]\n",
      "   [ 94  67  68]\n",
      "   [ 88  63  54]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 80 179  96]\n",
      "   [102 178 103]\n",
      "   [134 197 127]\n",
      "   ...\n",
      "   [ 93 169 126]\n",
      "   [ 97 171 128]\n",
      "   [102 169 128]]\n",
      "\n",
      "  [[ 84 180  99]\n",
      "   [ 95 179 102]\n",
      "   [121 194 121]\n",
      "   ...\n",
      "   [ 87 165 121]\n",
      "   [ 91 168 124]\n",
      "   [102 171 130]]\n",
      "\n",
      "  [[ 87 182 101]\n",
      "   [ 86 179 101]\n",
      "   [ 96 180 104]\n",
      "   ...\n",
      "   [ 91 170 125]\n",
      "   [ 90 169 126]\n",
      "   [ 97 168 126]]]\n",
      "\n",
      "\n",
      " [[[252 252 252]\n",
      "   [253 253 253]\n",
      "   [253 253 253]\n",
      "   ...\n",
      "   [210 210 210]\n",
      "   [211 211 211]\n",
      "   [211 211 211]]\n",
      "\n",
      "  [[253 253 253]\n",
      "   [252 252 252]\n",
      "   [251 251 251]\n",
      "   ...\n",
      "   [210 210 210]\n",
      "   [211 211 211]\n",
      "   [211 211 211]]\n",
      "\n",
      "  [[252 252 252]\n",
      "   [251 251 251]\n",
      "   [252 252 252]\n",
      "   ...\n",
      "   [210 210 210]\n",
      "   [210 210 210]\n",
      "   [210 210 210]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[182 182 178]\n",
      "   [145 134 134]\n",
      "   [154 123 136]\n",
      "   ...\n",
      "   [237 206 181]\n",
      "   [235 204 177]\n",
      "   [233 202 176]]\n",
      "\n",
      "  [[195 191 191]\n",
      "   [175 157 164]\n",
      "   [159 119 141]\n",
      "   ...\n",
      "   [227 223 213]\n",
      "   [227 224 211]\n",
      "   [225 222 210]]\n",
      "\n",
      "  [[205 194 200]\n",
      "   [187 161 174]\n",
      "   [144  95 125]\n",
      "   ...\n",
      "   [227 224 227]\n",
      "   [229 226 225]\n",
      "   [225 223 225]]]\n",
      "\n",
      "\n",
      " [[[107  88  84]\n",
      "   [103  81  78]\n",
      "   [ 73  51  50]\n",
      "   ...\n",
      "   [111  92  78]\n",
      "   [115  96  82]\n",
      "   [113  94  80]]\n",
      "\n",
      "  [[116  94  88]\n",
      "   [119  94  90]\n",
      "   [112  87  84]\n",
      "   ...\n",
      "   [115  93  83]\n",
      "   [112  92  79]\n",
      "   [109  90  76]]\n",
      "\n",
      "  [[128 104  94]\n",
      "   [125 100  93]\n",
      "   [130 104 100]\n",
      "   ...\n",
      "   [111  89  78]\n",
      "   [116  94  83]\n",
      "   [116  96  83]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 48  32  43]\n",
      "   [ 50  30  44]\n",
      "   [ 50  32  45]\n",
      "   ...\n",
      "   [163 120 104]\n",
      "   [185 136 116]\n",
      "   [188 133 111]]\n",
      "\n",
      "  [[ 44  31  41]\n",
      "   [ 46  29  41]\n",
      "   [ 54  37  48]\n",
      "   ...\n",
      "   [ 96  68  77]\n",
      "   [106  73  79]\n",
      "   [122  83  84]]\n",
      "\n",
      "  [[ 37  24  34]\n",
      "   [ 48  32  42]\n",
      "   [ 50  34  45]\n",
      "   ...\n",
      "   [ 92  68  91]\n",
      "   [ 91  66  89]\n",
      "   [ 87  61  83]]]] \n",
      "\n",
      "Test Image Landmarks:\n",
      "(2526, 128, 128, 3)\n",
      "[[[[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]\n",
      "\n",
      "  [[  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   ...\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]\n",
      "   [  0   0   0]]]\n",
      "\n",
      "\n",
      " [[[ 11  11  11]\n",
      "   [ 12  12  12]\n",
      "   [ 12  12  12]\n",
      "   ...\n",
      "   [ 36  36  36]\n",
      "   [ 38  38  38]\n",
      "   [ 48  48  48]]\n",
      "\n",
      "  [[ 12  12  12]\n",
      "   [ 12  12  12]\n",
      "   [ 12  12  12]\n",
      "   ...\n",
      "   [ 37  37  37]\n",
      "   [ 33  33  33]\n",
      "   [ 46  46  46]]\n",
      "\n",
      "  [[ 13  13  13]\n",
      "   [ 11  11  11]\n",
      "   [ 12  12  12]\n",
      "   ...\n",
      "   [ 19  19  19]\n",
      "   [ 18  18  18]\n",
      "   [ 31  31  31]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 10  10  10]\n",
      "   [ 10  10  10]\n",
      "   [ 11  11  11]\n",
      "   ...\n",
      "   [ 24  24  24]\n",
      "   [ 44  44  44]\n",
      "   [ 57  57  57]]\n",
      "\n",
      "  [[ 11  11  11]\n",
      "   [ 11  11  11]\n",
      "   [ 11  11  11]\n",
      "   ...\n",
      "   [ 33  33  33]\n",
      "   [ 49  49  49]\n",
      "   [ 68  68  68]]\n",
      "\n",
      "  [[ 12  12  12]\n",
      "   [ 11  11  11]\n",
      "   [ 12  12  12]\n",
      "   ...\n",
      "   [ 42  42  42]\n",
      "   [ 53  53  53]\n",
      "   [ 70  70  70]]]\n",
      "\n",
      "\n",
      " [[[ 12  24  14]\n",
      "   [108 124 110]\n",
      "   [127 147 129]\n",
      "   ...\n",
      "   [126 129 124]\n",
      "   [127 128 124]\n",
      "   [128 130 125]]\n",
      "\n",
      "  [[ 34  47  37]\n",
      "   [ 71  88  72]\n",
      "   [ 32  52  32]\n",
      "   ...\n",
      "   [127 129 127]\n",
      "   [126 128 125]\n",
      "   [128 130 125]]\n",
      "\n",
      "  [[ 33  46  35]\n",
      "   [ 26  44  27]\n",
      "   [ 17  37  16]\n",
      "   ...\n",
      "   [129 129 128]\n",
      "   [128 128 126]\n",
      "   [129 130 125]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 68  69  64]\n",
      "   [ 65  65  59]\n",
      "   [ 71  71  59]\n",
      "   ...\n",
      "   [246 249 247]\n",
      "   [253 254 254]\n",
      "   [251 251 251]]\n",
      "\n",
      "  [[ 85  78  74]\n",
      "   [ 78  69  62]\n",
      "   [ 90  80  65]\n",
      "   ...\n",
      "   [239 242 240]\n",
      "   [244 246 245]\n",
      "   [251 251 251]]\n",
      "\n",
      "  [[ 86  75  62]\n",
      "   [100  88  70]\n",
      "   [107  93  66]\n",
      "   ...\n",
      "   [244 246 245]\n",
      "   [239 241 240]\n",
      "   [247 247 247]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[244 242 245]\n",
      "   [245 243 246]\n",
      "   [245 243 246]\n",
      "   ...\n",
      "   [220 209 217]\n",
      "   [224 213 221]\n",
      "   [232 225 232]]\n",
      "\n",
      "  [[244 242 245]\n",
      "   [244 242 245]\n",
      "   [246 244 246]\n",
      "   ...\n",
      "   [229 216 223]\n",
      "   [225 212 219]\n",
      "   [219 210 216]]\n",
      "\n",
      "  [[244 242 245]\n",
      "   [246 243 244]\n",
      "   [248 244 245]\n",
      "   ...\n",
      "   [229 214 217]\n",
      "   [224 210 213]\n",
      "   [217 206 211]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[180 121  97]\n",
      "   [181 119  94]\n",
      "   [181 119  94]\n",
      "   ...\n",
      "   [186 124 101]\n",
      "   [183 118  95]\n",
      "   [177 110  85]]\n",
      "\n",
      "  [[172 113  88]\n",
      "   [175 112  87]\n",
      "   [174 110  85]\n",
      "   ...\n",
      "   [191 129 107]\n",
      "   [187 121 101]\n",
      "   [183 116  92]]\n",
      "\n",
      "  [[184 125 101]\n",
      "   [177 115  90]\n",
      "   [173 109  84]\n",
      "   ...\n",
      "   [196 134 113]\n",
      "   [193 127 108]\n",
      "   [190 122  99]]]\n",
      "\n",
      "\n",
      " [[[ 80  97  90]\n",
      "   [ 80  97  94]\n",
      "   [ 81  98  96]\n",
      "   ...\n",
      "   [ 11  26   6]\n",
      "   [ 17  34  15]\n",
      "   [ 24  42  22]]\n",
      "\n",
      "  [[ 85  95  97]\n",
      "   [ 85  95  97]\n",
      "   [ 85  95  97]\n",
      "   ...\n",
      "   [ 10  22   4]\n",
      "   [ 16  30  12]\n",
      "   [ 21  38  19]]\n",
      "\n",
      "  [[ 90  90 102]\n",
      "   [ 88  90 100]\n",
      "   [ 89  91  99]\n",
      "   ...\n",
      "   [ 20  32  14]\n",
      "   [ 26  38  21]\n",
      "   [ 31  44  26]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 55  51  52]\n",
      "   [ 57  51  50]\n",
      "   [ 56  49  48]\n",
      "   ...\n",
      "   [210 154 108]\n",
      "   [210 155 109]\n",
      "   [208 154 109]]\n",
      "\n",
      "  [[ 53  47  47]\n",
      "   [ 55  48  46]\n",
      "   [ 56  47  45]\n",
      "   ...\n",
      "   [222 164 117]\n",
      "   [219 161 116]\n",
      "   [215 158 113]]\n",
      "\n",
      "  [[ 51  45  45]\n",
      "   [ 56  46  45]\n",
      "   [ 58  48  47]\n",
      "   ...\n",
      "   [226 168 120]\n",
      "   [222 164 118]\n",
      "   [220 162 116]]]\n",
      "\n",
      "\n",
      " [[[127 134 123]\n",
      "   [130 143 133]\n",
      "   [132 149 137]\n",
      "   ...\n",
      "   [ 13  14  12]\n",
      "   [ 15  16  10]\n",
      "   [ 14  15  11]]\n",
      "\n",
      "  [[124 135 120]\n",
      "   [129 143 130]\n",
      "   [131 149 138]\n",
      "   ...\n",
      "   [ 13  17  11]\n",
      "   [ 13  16  11]\n",
      "   [ 15  16  11]]\n",
      "\n",
      "  [[122 132 123]\n",
      "   [130 141 132]\n",
      "   [131 149 140]\n",
      "   ...\n",
      "   [ 17  19  13]\n",
      "   [ 18  18  13]\n",
      "   [ 18  17  12]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 24  24  21]\n",
      "   [ 22  23  21]\n",
      "   [ 23  25  22]\n",
      "   ...\n",
      "   [ 67  37  14]\n",
      "   [ 66  35  14]\n",
      "   [ 63  34  15]]\n",
      "\n",
      "  [[ 23  24  21]\n",
      "   [ 24  24  22]\n",
      "   [ 25  26  23]\n",
      "   ...\n",
      "   [ 63  36  16]\n",
      "   [ 63  36  14]\n",
      "   [ 59  36  13]]\n",
      "\n",
      "  [[ 24  26  21]\n",
      "   [ 24  26  21]\n",
      "   [ 24  27  23]\n",
      "   ...\n",
      "   [ 63  38  17]\n",
      "   [ 61  40  16]\n",
      "   [ 60  38  16]]]] \n",
      "\n",
      "Train Labels:\n",
      "(11354,)\n",
      "['Angry' 'Angry' 'Angry' ... 'Surprise' 'Surprise' 'Surprise'] \n",
      "\n",
      "Test Labels:\n",
      "(2526,)\n",
      "['Angry' 'Angry' 'Angry' ... 'Surprise' 'Surprise' 'Surprise'] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:39:59.358139Z",
     "start_time": "2024-10-01T20:39:59.355067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Example landmarks of one image:\")\n",
    "print(train_image_landmarks[0].shape)\n",
    "print(train_image_landmarks[0])"
   ],
   "id": "f885cc333320f6f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example landmarks of one image:\n",
      "(128, 128, 3)\n",
      "[[[108  13  21]\n",
      "  [109  14  22]\n",
      "  [107  15  20]\n",
      "  ...\n",
      "  [ 75  41  35]\n",
      "  [ 71  34  30]\n",
      "  [ 68  28  26]]\n",
      "\n",
      " [[121  20  30]\n",
      "  [117  17  25]\n",
      "  [113  15  20]\n",
      "  ...\n",
      "  [ 75  43  35]\n",
      "  [ 73  36  32]\n",
      "  [ 70  32  29]]\n",
      "\n",
      " [[132  24  34]\n",
      "  [126  19  28]\n",
      "  [119  14  20]\n",
      "  ...\n",
      "  [ 79  46  39]\n",
      "  [ 77  43  39]\n",
      "  [ 76  38  35]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[145 104  80]\n",
      "  [140  99  75]\n",
      "  [129  89  64]\n",
      "  ...\n",
      "  [ 45  27  27]\n",
      "  [ 45  25  24]\n",
      "  [ 44  25  21]]\n",
      "\n",
      " [[165 120  89]\n",
      "  [158 113  83]\n",
      "  [147 102  72]\n",
      "  ...\n",
      "  [ 44  26  26]\n",
      "  [ 45  25  24]\n",
      "  [ 46  27  23]]\n",
      "\n",
      " [[177 128  95]\n",
      "  [169 120  87]\n",
      "  [158 113  79]\n",
      "  ...\n",
      "  [ 44  26  26]\n",
      "  [ 45  27  25]\n",
      "  [ 47  28  24]]]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Map the label names into actual labels\n",
    "Each label name should have its integer identificator.  \n",
    "From label names list get labels list, where label name has been replaced by its identificator"
   ],
   "id": "e11ed3988dafd9e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:40:08.096017Z",
     "start_time": "2024-10-01T20:40:08.092264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_label_names(label_names: np.array) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Map the label names, each label name will have its own identificator.\n",
    "    Replace the label names with their unique identifier.\n",
    "    :param label_names: list of label names\n",
    "    :return: an array of labels, where now the label names have been replaced by their unique identifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    mapped_labels = {}\n",
    "    \n",
    "    # map the unique label names\n",
    "    for label, unique_label_name in enumerate(np.unique(label_names)):\n",
    "        mapped_labels[unique_label_name] = label\n",
    "\n",
    "    # replace label name by its identificator\n",
    "    labels = np.array([mapped_labels[label_name] for label_name in label_names])\n",
    "    \n",
    "    return labels"
   ],
   "id": "877316d53a12fe4e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:40:09.393602Z",
     "start_time": "2024-10-01T20:40:09.387900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels = map_label_names(train_label_names)\n",
    "test_labels = map_label_names(test_label_names)\n",
    "\n",
    "print(\"Train labels:\")\n",
    "print(train_labels.shape)\n",
    "print(train_labels, '\\n')\n",
    "\n",
    "print(\"Test labels:\")\n",
    "print(test_labels.shape)\n",
    "print(test_labels)"
   ],
   "id": "8fedd15c8f86ad28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "(11354,)\n",
      "[0 0 0 ... 4 4 4] \n",
      "\n",
      "Test labels:\n",
      "(2526,)\n",
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encode the labels into one hot vectors\n",
   "id": "c5e254108e6ab149"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:40:16.840173Z",
     "start_time": "2024-10-01T20:40:16.836589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(labels: np.array, num_classes: int):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode the labels 1D vector into one-hot vectors encoding.\n",
    "    One hot encoded vector has all zeros, but only one 1.\n",
    "    \n",
    "    :param labels: 1D vector of labels \n",
    "        F.e.:\n",
    "        happy: 0\n",
    "        sad: 1\n",
    "        angry: 2\n",
    "        labels: [0, 0, 1, 1, 2]\n",
    "        \n",
    "    :param num_classes: number of unique classes - of unique labels (f.e. 3 - happy, sad, and angry)\n",
    "    :return: a 2D one hot encoded array.\n",
    "    \n",
    "            F.e.:\n",
    "            labels = [0, 0, 1, 1, 2]\n",
    "            one_hot = \n",
    "                [\n",
    "                    [1 0 0]\n",
    "                    [1 0 0]\n",
    "                    [0 1 0]\n",
    "                    [0 1 0]\n",
    "                    [0 0 1]\n",
    "                ]\n",
    "            - shape of one_hot: (n_labels, unique_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # an array full of zeros of shape (num_labels, num_classes) \n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    \n",
    "    # set the 1 to appropriate labels\n",
    "    for n_row, label in enumerate(labels):\n",
    "        one_hot[n_row, label] = 1\n",
    "    \n",
    "    return one_hot"
   ],
   "id": "457f69a8205cc79c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:40:19.688785Z",
     "start_time": "2024-10-01T20:40:19.681711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = len(np.unique(test_labels))\n",
    "\n",
    "train_labels_one_hot = one_hot_encode(train_labels, num_classes)\n",
    "test_labels_one_hot = one_hot_encode(test_labels, num_classes)\n",
    "\n",
    "print(\"Train labels one-hot:\")\n",
    "print(train_labels_one_hot.shape)\n",
    "print(train_labels_one_hot, '\\n')\n",
    "\n",
    "print(\"Test labels one-hot:\")\n",
    "print(test_labels_one_hot.shape)\n",
    "print(test_labels_one_hot, '\\n')"
   ],
   "id": "17ea33015821beae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels one-hot:\n",
      "(11354, 5)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]] \n",
      "\n",
      "Test labels one-hot:\n",
      "(2526, 5)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final Representation of Data\n",
    "\n",
    "\n",
    "Training and testing sets:  \n",
    "- image arrays: train_images and test_images\n",
    "- one-hot encoded vector arrays: train_labels_one_hot, test_labels_one_hot"
   ],
   "id": "7453b4f228769d08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:41:00.132502Z",
     "start_time": "2024-10-01T20:41:00.128484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train Images (first):\")\n",
    "print(train_image_landmarks.shape)\n",
    "print(train_image_landmarks[0], '\\n')\n",
    "\n",
    "print(\"Test Images (first):\")\n",
    "print(test_image_landmarks.shape)\n",
    "print(test_image_landmarks[0], '\\n')\n",
    "\n",
    "print(\"Train Labels one-hot:\")\n",
    "print(train_labels_one_hot.shape)\n",
    "print(train_labels_one_hot, '\\n')\n",
    "\n",
    "print(\"Test Labels one-hot:\")\n",
    "print(test_labels_one_hot.shape)\n",
    "print(test_labels_one_hot, '\\n')"
   ],
   "id": "3e494c38a77f17dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images (first):\n",
      "(11354, 128, 128, 3)\n",
      "[[[108  13  21]\n",
      "  [109  14  22]\n",
      "  [107  15  20]\n",
      "  ...\n",
      "  [ 75  41  35]\n",
      "  [ 71  34  30]\n",
      "  [ 68  28  26]]\n",
      "\n",
      " [[121  20  30]\n",
      "  [117  17  25]\n",
      "  [113  15  20]\n",
      "  ...\n",
      "  [ 75  43  35]\n",
      "  [ 73  36  32]\n",
      "  [ 70  32  29]]\n",
      "\n",
      " [[132  24  34]\n",
      "  [126  19  28]\n",
      "  [119  14  20]\n",
      "  ...\n",
      "  [ 79  46  39]\n",
      "  [ 77  43  39]\n",
      "  [ 76  38  35]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[145 104  80]\n",
      "  [140  99  75]\n",
      "  [129  89  64]\n",
      "  ...\n",
      "  [ 45  27  27]\n",
      "  [ 45  25  24]\n",
      "  [ 44  25  21]]\n",
      "\n",
      " [[165 120  89]\n",
      "  [158 113  83]\n",
      "  [147 102  72]\n",
      "  ...\n",
      "  [ 44  26  26]\n",
      "  [ 45  25  24]\n",
      "  [ 46  27  23]]\n",
      "\n",
      " [[177 128  95]\n",
      "  [169 120  87]\n",
      "  [158 113  79]\n",
      "  ...\n",
      "  [ 44  26  26]\n",
      "  [ 45  27  25]\n",
      "  [ 47  28  24]]] \n",
      "\n",
      "Test Images (first):\n",
      "(2526, 128, 128, 3)\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]] \n",
      "\n",
      "Train Labels one-hot:\n",
      "(11354, 5)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]] \n",
      "\n",
      "Test Labels one-hot:\n",
      "(2526, 5)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Helpful Functions\n",
    "Functions that have nothing to do with CNN.  \n",
    "Helpful functions to make the program run smoothly."
   ],
   "id": "917b70cd4bffacdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd60adc58b052c7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convolutional Layers",
   "id": "722a9283be8dba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Custom Errors\n",
    "Made custom errors, which can be raised in the convolutional layer."
   ],
   "id": "15f83df2028defab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:57:26.766417Z",
     "start_time": "2024-09-27T20:57:26.764223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InvalidNumberOfFilters(Exception):\n",
    "    \"\"\" Invalid number of filters specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid number of filters specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class InvalidFilterSize(Exception):\n",
    "    \"\"\" Invalid filter size specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid filter size specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "        \n",
    "class InvalidNumberOfChannels(Exception):\n",
    "    \"\"\" Invalid number of channels specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid number of channels specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "        \n",
    "class InvalidPadding(Exception):\n",
    "    \"\"\" Invalid padding specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid padding specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "\n",
    "class InvalidStride(Exception):\n",
    "    \"\"\" Invalid stride specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid stride specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ],
   "id": "7a273879b5674dcd",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:41:15.808542Z",
     "start_time": "2024-10-01T20:41:15.797728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvLayer:\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters: int, filter_size: int, num_channels: int):\n",
    "        \"\"\"\n",
    "        Function constructor, initialize the filter array.\n",
    "        \n",
    "        :param num_filters: number of filters in the convolutional layer\n",
    "        :param filter_size: size of the filter, num_channels x filter_size x filter_size\n",
    "        :param num_channels: depth of the filter - kernel\n",
    "        :raises \n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(num_filters, int):\n",
    "            raise ValueError(\"Number of filters \\\"num_filters\\\" must be an integer!\")\n",
    "        elif num_filters < 1:\n",
    "            raise InvalidNumberOfFilters(f\"Number of filters \\\"num_filters\\\" must be at least 1, \\\"num_filters={num_filters}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(filter_size, int):\n",
    "            raise ValueError(\"Filter - kernel size \\\"filter_size\\\" must be an integer!\")\n",
    "        elif filter_size < 2:\n",
    "            raise InvalidFilterSize(f\"Filter - kernel size \\\"filter_size\\\" must be at least 2, \\\"filter_size={filter_size}\\\" specified instead!\")\n",
    "            \n",
    "        if not isinstance(num_channels, int):\n",
    "            raise ValueError(\"Number of channels \\\"num_channels\\\" must be an integer!\")\n",
    "        elif num_channels < 1:\n",
    "            raise InvalidNumberOfChannels(f\"Number of channels \\\"num_channels\\\" must be at least 1, \\\"num_channels={num_channels}\\\" was specified instead!\")\n",
    "            \n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # creates a 4D numpy array\n",
    "        # filter is of size (c x f x f), where f is the filter_size, and c is the num_channels\n",
    "        # (c x f x f) because image size is expected to be (c x height x width)\n",
    "        self.filters = np.random.randn(num_filters, num_channels, filter_size, filter_size) / filter_size ** 2\n",
    "    \n",
    "    \n",
    "    def shift_filter_window(self, image: np.array, stride: int) -> (np.array, int, int):\n",
    "        \n",
    "        \"\"\"\n",
    "        Shift the filter window to get all the regions in the image.\n",
    "        These regions are for the convolutional operations with the kernel - filter.\n",
    "        \n",
    "        :param image: image from which to get all the regions from.\n",
    "        :param stride: size of the stride, shift size - how big a step/shift\n",
    "        :return: a region (where input image and kernel intersect) which is a 2D array, and (i, j) coordinates of the region  \n",
    "        \"\"\"\n",
    "        \n",
    "        _, height, width = image.shape\n",
    "        \n",
    "        # shift the filter window, to process all regions in the input image\n",
    "        for i in range(0, height - self.filter_size + 1, stride):\n",
    "            for j in range(0, width - self.filter_size + 1, stride):\n",
    "                # extract the region of the image where the filter is applied\n",
    "                region = image[:, i: (i + self.filter_size), j: (j + self.filter_size)]\n",
    "                yield region, i, j  # what yield does that it returns a value, but this function has to be interated to get next() values\n",
    "                \n",
    "                \n",
    "    def forward(self, image: np.array, stride: int = 1, padding: int = 0) -> np.array:\n",
    "\n",
    "        \"\"\"\n",
    "        Perform forward pass through the convolutional layer.\n",
    "        Perform convolutional operations with the kernel - filter \n",
    "        \n",
    "        :param image: image to perform convolutional operation on \n",
    "        :param padding: padding applied to the image\n",
    "        :param stride: stride applied to the convolutional operation\n",
    "        :return: a convoluted image\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(stride, int):\n",
    "            raise ValueError(\"Stride must be an integer!\")\n",
    "        elif stride < 1:\n",
    "            raise InvalidStride(f\"Stride must be at least 1, \\\"stride={stride}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(padding, int):\n",
    "            raise ValueError(\"Padding must be an integer!\")\n",
    "        elif padding < 0:\n",
    "            raise InvalidPadding(f\"Padding must be at least 0, \\\"padding={padding}\\\" specified instead!\")\n",
    "        \n",
    "        # if image is 2D (single channel, number of channels is 1), expand from (height x width) into (1 x height x width) so it has a depth=1\n",
    "        if image.ndim == 2:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "        padded_image = self.pad_image(image, padding)  # apply padding to the image\n",
    "        _, height, width = image.shape  # get only height and width, image is of size (n_channels x height x width)\n",
    "        \n",
    "        # expected output height and width\n",
    "        output_height = (height + 2 * padding - self.filter_size) // stride + 1\n",
    "        output_width = (width + 2 * padding - self.filter_size) // stride + 1\n",
    "        \n",
    "        output = np.zeros((self.num_filters, output_height, output_width))\n",
    "\n",
    "        for region, i, j in self.shift_filter_window(padded_image, stride):\n",
    "            for n_filter in range(self.num_filters):\n",
    "                # perform convolution on the region using the filter\n",
    "                output[n_filter, i // stride, j // stride] = self.conv(region, self.filters[n_filter]) \n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv(image_region: np.array, kernel: np.array) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform convolutional operation on the image region using the filter.\n",
    "        Convolution operation is sum(image_region * filter)\n",
    "        \n",
    "        :param image_region: region to perform convolutional operation on\n",
    "        :param kernel: filter - kernel to perform convolutional operation with\n",
    "        :return: convolution result\n",
    "        \"\"\"\n",
    "        \n",
    "        result = 0\n",
    "  \n",
    "        # perform convolution for each channel separately and sum the result\n",
    "        for channel in range(image_region.shape[0]):  # loop over the channels\n",
    "            conv_result = convolve2d(image_region[channel], kernel[channel], mode=\"valid\")\n",
    "            result += conv_result.item()  # extract the scalar value if it is a 1 x 1 array\n",
    "\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def pad_image(image: np.array, padding: int) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply zero-padding to the input image with multiple channels.\n",
    "        \n",
    "        :param image: image to apply padding on\n",
    "        :param padding: padding applied to the image\n",
    "        :return: padded image\n",
    "        \"\"\"\n",
    "        \n",
    "        if padding > 0:\n",
    "            # apply padding to the height and width dimensions (1 and 2), keeping channels dimension intact\n",
    "            padded_image = np.pad(image, ((0, 0), (padding, padding), (padding, padding)), mode=\"constant\")\n",
    "        else:\n",
    "            padded_image = image\n",
    "            \n",
    "        return padded_image\n",
    "        "
   ],
   "id": "7e0f8dfc5486b5e2",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:41:43.507493Z",
     "start_time": "2024-10-01T20:41:43.412928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of using one convolutional layer, using only forward pass\n",
    "\n",
    "conv_layer1 = ConvLayer(num_filters=3, filter_size=4, num_channels=1)\n",
    "\n",
    "batch_size = 300\n",
    "convolved_images = []\n",
    "\n",
    "for i in range(0, train_image_landmarks.shape[0], batch_size):  # loop over all the number of images (28709)\n",
    "    \n",
    "    print(f\"{(i // batch_size) + 1}. batch {i}/{train_image_landmarks.shape[0]}\")\n",
    "    \n",
    "    convolved_batch = []\n",
    "    batch_images = train_image_landmarks[i: i + batch_size]  # Get the i-th image (shape 48x48)\n",
    "    \n",
    "    for image in batch_images:\n",
    "        convolved_image = conv_layer1.forward(image, padding=0, stride=1)\n",
    "        convolved_batch.append(convolved_image)\n",
    "    \n",
    "    convolved_images.extend(np.array(convolved_batch))\n",
    "\n",
    "convolved_images = np.array(convolved_images)"
   ],
   "id": "ef5198811a4a8416",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. batch 0/11354\n",
      "2. batch 300/11354\n",
      "3. batch 600/11354\n",
      "4. batch 900/11354\n",
      "5. batch 1200/11354\n",
      "6. batch 1500/11354\n",
      "7. batch 1800/11354\n",
      "8. batch 2100/11354\n",
      "9. batch 2400/11354\n",
      "10. batch 2700/11354\n",
      "11. batch 3000/11354\n",
      "12. batch 3300/11354\n",
      "13. batch 3600/11354\n",
      "14. batch 3900/11354\n",
      "15. batch 4200/11354\n",
      "16. batch 4500/11354\n",
      "17. batch 4800/11354\n",
      "18. batch 5100/11354\n",
      "19. batch 5400/11354\n",
      "20. batch 5700/11354\n",
      "21. batch 6000/11354\n",
      "22. batch 6300/11354\n",
      "23. batch 6600/11354\n",
      "24. batch 6900/11354\n",
      "25. batch 7200/11354\n",
      "26. batch 7500/11354\n",
      "27. batch 7800/11354\n",
      "28. batch 8100/11354\n",
      "29. batch 8400/11354\n",
      "30. batch 8700/11354\n",
      "31. batch 9000/11354\n",
      "32. batch 9300/11354\n",
      "33. batch 9600/11354\n",
      "34. batch 9900/11354\n",
      "35. batch 10200/11354\n",
      "36. batch 10500/11354\n",
      "37. batch 10800/11354\n",
      "38. batch 11100/11354\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:41:50.649780Z",
     "start_time": "2024-10-01T20:41:50.644317Z"
    }
   },
   "cell_type": "code",
   "source": "convolved_images.shape",
   "id": "cce03ea6e23eeaeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11354, 3, 125, 0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c5b28d6583bc3714"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
