{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ! Important Note !\n",
    "\n",
    "Commented code was used to extract facial landmarks from the images.  \n",
    "BUT in this section we do not use facial landmarks as an input but whole normalized images.  \n",
    "It was just a test if I could extract facial landmarks from the images.  \n",
    "\n",
    "Facial landmarks are used in a different CNN algorithm. THis algorithm will not be from sracth like this one.  \n",
    "It will use all the helpful frameworks to create the best model. In addition, this CNN will recognize facial expressions live in a webcam.  \n",
    "(not like this one, this one only recognizes facial expressions from images, not live capture)"
   ],
   "id": "fe1368e4c8a47c02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "4726bb54851a53f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:40.477838Z",
     "start_time": "2024-10-03T18:05:39.726911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "from PIL import Image\n",
    "# import mediapipe as mp  # to get facial landmarks from the face images"
   ],
   "id": "db3498eb397da313",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Constants",
   "id": "dfcf95a14f20c9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:40.485950Z",
     "start_time": "2024-10-03T18:05:40.484286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data_greyscale\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")"
   ],
   "id": "9f7fcc1d59d4d11e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convolutional Neural Network",
   "id": "7edbbe1dbb3989b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reading Data\n",
    "Data are face images.  \n",
    "If image is in grayscale, there is only 1 channel, for RGB image there would be 3 channels.  \n",
    "\n",
    "1. Read images, remember the image's label (f.e. happy, sad, ...).\n",
    "2. Map the label names list, each label name will now have its own identificator. (f.e. happy = 0, sad = 1, ...).\n",
    "3. Convert the labels into a one-hot encoded vector array."
   ],
   "id": "c08143d6f64ad20f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:40.490404Z",
     "start_time": "2024-10-03T18:05:40.488597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # initialize MediaPipe Face Mesh model to extract facial landmarks\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "# face_mesh = mp_face_mesh.FaceMesh(\n",
    "#     static_image_mode=True,  # static_image_mode=True since we extract from images and not live video\n",
    "#     min_detection_confidence=0.5,  # lowering the threshhold, to detect landmarks in difficult images\n",
    "#     min_tracking_confidence=0.5\n",
    "# )\n",
    "# \n",
    "# def extract_landmarks(image_array: np.array, is_greyscale: bool) -> np.array:\n",
    "#     \n",
    "#     \"\"\"\n",
    "#     Extract facial landmarks from the image using MediaPipe.\n",
    "#     :param image_array: 2D numpy array representing the image\n",
    "#     :param is_greyscale: True if the image is greyscale, False image is in RGB\n",
    "#     :return: numpy array containing the normalized facial landmarks\n",
    "#     \"\"\"\n",
    "#     \n",
    "#     if is_greyscale:  # convert image array to RGB format, since MediaPipe works with RGB\n",
    "#         image_rgb = np.stack((image_array,) * 3, axis=-1)  # convert grayscale to RGB format\n",
    "#     else:\n",
    "#         image_rgb = image_array  # image already is RGB\n",
    "#     \n",
    "#     results = face_mesh.process(image_rgb)  # get all the landmarks from the image\n",
    "# \n",
    "#     if not results.multi_face_landmarks:\n",
    "#         return None  # no face detected, return None\n",
    "#         \n",
    "#     landmarks = results.multi_face_landmarks[0].landmark  # extract the landmarks from the first detected face\n",
    "#     \n",
    "#     # convert the landmark list into an array, the landmark has already normalized values between 0 and 1\n",
    "#     landmarks_array = np.array([(lm.x, lm.y) for lm in landmarks])\n",
    "# \n",
    "#     return landmarks_array"
   ],
   "id": "7a978775470d8c7c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:40.496803Z",
     "start_time": "2024-10-03T18:05:40.493318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data(data_dir: str, is_greyscale: bool, image_size=(48, 48)) -> (np.array, np.array):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reading images from data_dir.\n",
    "    Each image is processed and converted into a numpy array.\n",
    "    \n",
    "    Expected directory tree:\n",
    "    - data_dir\n",
    "        - facial_expression_dir1\n",
    "            - image1\n",
    "            - image2\n",
    "            - ...\n",
    "        - facial_expression_dir2 \n",
    "        - facial_expression_dir3\n",
    "        - ...\n",
    "        - facial_expression_dirN\n",
    "        \n",
    "    :param data_dir: directory from which the images are processed\n",
    "    :param image_size: size of the images, default is 48x48\n",
    "    :param is_greyscale: True if the images are greyscale, False images are in RGB\n",
    "    :return: two numpy arrays, first numpy array has stored all the facial landmarks images, and second numpy array has stored all the label names\n",
    "    \"\"\"\n",
    "    \n",
    "    # failed_image_landmarks = {}\n",
    "    # image_landmarks = []\n",
    "    images = []\n",
    "    label_names = []\n",
    "\n",
    "    for expression_dirname in sorted(os.listdir(data_dir)):  \n",
    "        # get every directory, this directory contains images of facial expressions \n",
    "        \n",
    "        expression_dir = os.path.join(data_dir, expression_dirname)\n",
    "        \n",
    "        if not os.path.isdir(expression_dir):  # process only directories, skip non-directories - files\n",
    "            continue\n",
    "            \n",
    "        # failed_image_landmarks[expression_dirname] = {\n",
    "        #     \"all_images_count\": len(os.listdir(expression_dir)),\n",
    "        #     \"failed_images\": []\n",
    "        # }\n",
    "        \n",
    "        for expression_image in os.listdir(expression_dir):\n",
    "            # get every image in the directory\n",
    "            image_path = os.path.join(expression_dir, expression_image)\n",
    "            \n",
    "            try:\n",
    "                if is_greyscale:\n",
    "                    image = Image.open(image_path).convert('L')  # L mode, because images are grayscaled\n",
    "                else:\n",
    "                    image = Image.open(image_path).convert(\"RGB\")  # RGB mode, because images are not grayscaled, and therefore they are RGB \n",
    "                image.verify()  # verify the integrity of the image\n",
    "                \n",
    "                image = image.resize(image_size)  # resize the image to the image_size\n",
    "                \n",
    "                # convert image into an array, image is a 2D array\n",
    "                image_array = np.array(image) / 255.0\n",
    "                \n",
    "                images.append(image_array)\n",
    "                label_names.append(expression_dirname)  # directory name is already a label name of a facial expression\n",
    "                \n",
    "                # # extract normlized facial landmarks\n",
    "                # landmarks = extract_landmarks(image_array, is_greyscale)\n",
    "                # \n",
    "                # if landmarks is not None:\n",
    "                #     # successfully the facial landmarks were extracted from the image\n",
    "                #     image_landmarks.append(landmarks)\n",
    "                #     label_names.append(expression_dirname)  # directory name is already a label name of a facial expression\n",
    "                # else:\n",
    "                #     # failed to get facial landmarks, add the failed image into the log of failed landmarks\n",
    "                #     failed_image_landmarks[expression_dirname][\"failed_images\"].append(os.path.split(image_path)[-1])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image: {image_path}, error: {e}\")\n",
    "                continue  # skip to the next image\n",
    "            \n",
    "    # # log images from which facial landmarks extraction has failed\n",
    "    # if failed_image_landmarks:\n",
    "    #     print(\"Failed image landmarks: \\n\")\n",
    "    #     \n",
    "    #     all_images_count = 0\n",
    "    #     all_failed_images_count = 0\n",
    "    #     \n",
    "    #     for expression, info_dict in failed_image_landmarks.items():\n",
    "    #         \n",
    "    #         all_expression_images_count = info_dict[\"all_images_count\"]\n",
    "    #         images_failed = info_dict[\"failed_images\"]\n",
    "    #         \n",
    "    #         n_failed_images = len(images_failed)\n",
    "    #         fail_ratio = round(n_failed_images / all_expression_images_count * 100, 2)\n",
    "    #         \n",
    "    #         print(f\"{expression.upper()} - FAILED: {n_failed_images}/{all_expression_images_count} - {fail_ratio}%\")\n",
    "    #         for image_failed in images_failed:\n",
    "    #             print(f\"\\t - {image_failed}\")\n",
    "    #         print(\"\")\n",
    "    #         \n",
    "    #         all_images_count += all_expression_images_count\n",
    "    #         all_failed_images_count += n_failed_images\n",
    "    #     \n",
    "    #     overall_fail_ratio = round(all_failed_images_count / all_images_count * 100, 2)\n",
    "    #     print(f\"OVERALL - FAILED: {all_failed_images_count}/{all_images_count} - {overall_fail_ratio}%\")\n",
    "\n",
    "    # convert images and labels list into numpy arrays and return them\n",
    "    return np.array(images), np.array(label_names)   \n",
    "    \n",
    "    # return np.array(image_landmarks), np.array(label_names)            "
   ],
   "id": "625c2057213acb0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:22.010601Z",
     "start_time": "2024-10-03T18:10:16.646416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_images, train_label_names = get_data(TRAIN_DIR, is_greyscale=True)\n",
    "# train_image_landmarks, train_label_names = get_data(TRAIN_DIR, is_greyscale=False, image_size=(96, 96))"
   ],
   "id": "39db644581ce34dd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:24.185970Z",
     "start_time": "2024-10-03T18:10:23.033947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_images, test_label_names = get_data(TEST_DIR, is_greyscale=True)\n",
    "# test_image_landmarks, test_label_names = get_data(TEST_DIR, is_greyscale=False, image_size=(96, 96))"
   ],
   "id": "650de7374c5aa3b9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:25.667918Z",
     "start_time": "2024-10-03T18:10:25.662845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train Images:\")\n",
    "print(train_images.shape)\n",
    "print(train_images, '\\n')\n",
    "\n",
    "print(\"Test Images:\")\n",
    "print(test_images.shape)\n",
    "print(test_images, '\\n')\n",
    "\n",
    "print(\"Train Labels:\")\n",
    "print(train_label_names.shape)\n",
    "print(train_label_names, '\\n')\n",
    "\n",
    "print(\"Test Labels:\")\n",
    "print(test_label_names.shape)\n",
    "print(test_label_names, '\\n')"
   ],
   "id": "1346401500fa0883",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images:\n",
      "(28709, 48, 48)\n",
      "[[[0.70980392 0.70196078 0.69411765 ... 0.71372549 0.71372549 0.71372549]\n",
      "  [0.70196078 0.69803922 0.69019608 ... 0.70196078 0.69411765 0.68627451]\n",
      "  [0.70196078 0.69803922 0.69019608 ... 0.67843137 0.70196078 0.7254902 ]\n",
      "  ...\n",
      "  [0.76862745 0.70980392 0.74901961 ... 0.90196078 0.89411765 0.80392157]\n",
      "  [0.76078431 0.72941176 0.78431373 ... 0.89019608 0.87058824 0.91372549]\n",
      "  [0.77647059 0.77254902 0.83137255 ... 0.88627451 0.85882353 0.95294118]]\n",
      "\n",
      " [[0.08235294 0.07058824 0.10588235 ... 0.32941176 0.20392157 0.24705882]\n",
      "  [0.08235294 0.08235294 0.10980392 ... 0.34509804 0.28235294 0.36078431]\n",
      "  [0.09019608 0.10980392 0.12941176 ... 0.4        0.41176471 0.45882353]\n",
      "  ...\n",
      "  [0.99607843 0.99215686 1.         ... 0.63921569 0.61960784 0.61176471]\n",
      "  [1.         1.         1.         ... 0.59607843 0.64705882 0.59607843]\n",
      "  [0.99607843 1.         0.99215686 ... 0.61568627 0.57647059 0.56078431]]\n",
      "\n",
      " [[0.16078431 0.24705882 0.33333333 ... 0.13333333 0.1372549  0.1254902 ]\n",
      "  [0.14117647 0.21960784 0.30196078 ... 0.1254902  0.10980392 0.13333333]\n",
      "  [0.12941176 0.19607843 0.3254902  ... 0.15686275 0.09019608 0.12941176]\n",
      "  ...\n",
      "  [0.51764706 0.5372549  0.52941176 ... 0.33333333 0.29411765 0.32156863]\n",
      "  [0.49411765 0.54117647 0.58039216 ... 0.35294118 0.31764706 0.30588235]\n",
      "  [0.48627451 0.54509804 0.59607843 ... 0.37647059 0.33333333 0.31764706]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.98431373 0.98431373 0.97254902 ... 0.97254902 0.98431373 0.98039216]\n",
      "  [0.98431373 0.98431373 0.99607843 ... 0.98039216 0.98039216 0.98431373]\n",
      "  [0.98039216 0.98039216 0.98431373 ... 0.97254902 0.97647059 0.96862745]\n",
      "  ...\n",
      "  [0.98823529 0.97254902 0.98431373 ... 0.92941176 0.91764706 0.9254902 ]\n",
      "  [0.98039216 0.98431373 0.98431373 ... 0.92941176 0.92941176 0.94117647]\n",
      "  [0.99215686 0.98431373 0.98431373 ... 0.9372549  0.94117647 0.94509804]]\n",
      "\n",
      " [[0.99215686 0.99215686 1.         ... 0.99215686 0.99215686 1.        ]\n",
      "  [0.99215686 0.99607843 0.98823529 ... 0.99607843 0.99215686 0.99607843]\n",
      "  [1.         1.         1.         ... 0.99607843 0.99215686 1.        ]\n",
      "  ...\n",
      "  [0.09411765 0.08627451 0.07843137 ... 0.13333333 0.14901961 0.13333333]\n",
      "  [0.09411765 0.05882353 0.09019608 ... 0.14509804 0.1372549  0.14509804]\n",
      "  [0.09803922 0.08235294 0.05882353 ... 0.14509804 0.13333333 0.14509804]]\n",
      "\n",
      " [[0.96862745 0.99215686 0.99607843 ... 0.02745098 0.03529412 0.03529412]\n",
      "  [0.98039216 0.69803922 0.64705882 ... 0.05882353 0.03921569 0.07058824]\n",
      "  [0.83529412 0.62352941 0.9254902  ... 0.09019608 0.02745098 0.05490196]\n",
      "  ...\n",
      "  [0.99215686 1.         1.         ... 0.25098039 0.34901961 0.40784314]\n",
      "  [0.99215686 0.91764706 0.85098039 ... 0.23921569 0.36470588 0.44705882]\n",
      "  [0.61568627 0.56470588 0.5254902  ... 0.25098039 0.44313725 0.57254902]]] \n",
      "\n",
      "Test Images:\n",
      "(7178, 48, 48)\n",
      "[[[0.3254902  0.29411765 0.24313725 ... 0.12156863 0.12156863 0.12156863]\n",
      "  [0.23921569 0.20784314 0.20784314 ... 0.12156863 0.12156863 0.1254902 ]\n",
      "  [0.16862745 0.14901961 0.13333333 ... 0.14117647 0.11764706 0.10588235]\n",
      "  ...\n",
      "  [0.08235294 0.07843137 0.07843137 ... 0.03921569 0.04313725 0.0627451 ]\n",
      "  [0.09411765 0.08627451 0.07843137 ... 0.06666667 0.04313725 0.02745098]\n",
      "  [0.08627451 0.08235294 0.05882353 ... 0.04705882 0.03529412 0.03921569]]\n",
      "\n",
      " [[0.49803922 0.49411765 0.48235294 ... 0.24313725 0.24705882 0.19607843]\n",
      "  [0.50588235 0.49803922 0.49019608 ... 0.23921569 0.25490196 0.21960784]\n",
      "  [0.51372549 0.50196078 0.49803922 ... 0.23529412 0.21960784 0.22352941]\n",
      "  ...\n",
      "  [0.80784314 0.77254902 0.81176471 ... 0.06666667 0.05490196 0.11372549]\n",
      "  [0.78823529 0.76862745 0.87058824 ... 0.12941176 0.05490196 0.10196078]\n",
      "  [0.74901961 0.80392157 0.9372549  ... 0.50196078 0.38039216 0.30588235]]\n",
      "\n",
      " [[0.98039216 0.98431373 0.98431373 ... 0.98039216 0.98039216 0.97647059]\n",
      "  [0.98039216 0.98431373 0.98039216 ... 0.96078431 0.98431373 0.98431373]\n",
      "  [0.97647059 0.98431373 0.98039216 ... 1.         0.96078431 0.97647059]\n",
      "  ...\n",
      "  [0.59215686 0.58823529 0.60784314 ... 0.25882353 0.41960784 0.37254902]\n",
      "  [0.58823529 0.59607843 0.62352941 ... 0.34901961 0.41960784 0.3372549 ]\n",
      "  [0.6        0.6        0.61960784 ... 0.42352941 0.38431373 0.33333333]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.03921569 0.21568627 0.23921569 ... 0.23137255 0.02745098 0.00784314]\n",
      "  [0.20784314 0.25490196 0.2627451  ... 0.25098039 0.10980392 0.        ]\n",
      "  [0.25490196 0.25882353 0.2745098  ... 0.27058824 0.22352941 0.00784314]\n",
      "  ...\n",
      "  [0.00784314 0.         0.01568627 ... 0.01176471 0.01960784 0.01176471]\n",
      "  [0.01176471 0.01176471 0.         ... 0.00784314 0.01960784 0.00392157]\n",
      "  [0.00392157 0.01568627 0.00784314 ... 0.33333333 0.03137255 0.02745098]]\n",
      "\n",
      " [[0.98431373 0.57254902 0.27843137 ... 0.29411765 0.79215686 0.99607843]\n",
      "  [0.91764706 0.50196078 0.27843137 ... 0.23921569 0.72941176 1.        ]\n",
      "  [0.83921569 0.44705882 0.29803922 ... 0.2627451  0.71764706 0.99607843]\n",
      "  ...\n",
      "  [1.         1.         1.         ... 1.         0.99215686 1.        ]\n",
      "  [1.         1.         1.         ... 1.         0.99607843 1.        ]\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]]\n",
      "\n",
      " [[0.69411765 0.72156863 0.54901961 ... 0.70588235 0.71372549 0.71764706]\n",
      "  [0.69803922 0.74901961 0.4        ... 0.71764706 0.70588235 0.70980392]\n",
      "  [0.69803922 0.74117647 0.27843137 ... 0.71764706 0.71372549 0.72156863]\n",
      "  ...\n",
      "  [0.4627451  0.80392157 0.8        ... 0.29019608 0.33333333 0.38823529]\n",
      "  [0.36862745 0.77254902 0.81176471 ... 0.43921569 0.39607843 0.35294118]\n",
      "  [0.2        0.61960784 0.89019608 ... 0.26666667 0.40784314 0.42352941]]] \n",
      "\n",
      "Train Labels:\n",
      "(28709,)\n",
      "['angry' 'angry' 'angry' ... 'surprise' 'surprise' 'surprise'] \n",
      "\n",
      "Test Labels:\n",
      "(7178,)\n",
      "['angry' 'angry' 'angry' ... 'surprise' 'surprise' 'surprise'] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:32.933660Z",
     "start_time": "2024-10-03T18:10:32.930779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Example of one image:\")\n",
    "print(train_images[0].shape)\n",
    "print(train_images[0])"
   ],
   "id": "563a461575270b07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of one image:\n",
      "(48, 48)\n",
      "[[0.70980392 0.70196078 0.69411765 ... 0.71372549 0.71372549 0.71372549]\n",
      " [0.70196078 0.69803922 0.69019608 ... 0.70196078 0.69411765 0.68627451]\n",
      " [0.70196078 0.69803922 0.69019608 ... 0.67843137 0.70196078 0.7254902 ]\n",
      " ...\n",
      " [0.76862745 0.70980392 0.74901961 ... 0.90196078 0.89411765 0.80392157]\n",
      " [0.76078431 0.72941176 0.78431373 ... 0.89019608 0.87058824 0.91372549]\n",
      " [0.77647059 0.77254902 0.83137255 ... 0.88627451 0.85882353 0.95294118]]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:47.328579Z",
     "start_time": "2024-10-03T18:05:47.327101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"Train Image Landmarks:\")\n",
    "# print(train_image_landmarks.shape)\n",
    "# print(train_image_landmarks, '\\n')\n",
    "# \n",
    "# print(\"Test Image Landmarks:\")\n",
    "# print(test_image_landmarks.shape)\n",
    "# print(test_image_landmarks, '\\n')\n",
    "# \n",
    "# print(\"Train Labels:\")\n",
    "# print(train_label_names.shape)\n",
    "# print(train_label_names, '\\n')\n",
    "# \n",
    "# print(\"Test Labels:\")\n",
    "# print(test_label_names.shape)\n",
    "# print(test_label_names, '\\n')"
   ],
   "id": "c2812cb7cf9c7225",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:47.334594Z",
     "start_time": "2024-10-03T18:05:47.333323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(\"Example landmarks of one image:\")\n",
    "# print(train_image_landmarks[0].shape)\n",
    "# print(train_image_landmarks[0])"
   ],
   "id": "f885cc333320f6f0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Map the label names into actual labels\n",
    "Each label name should have its integer identificator.  \n",
    "From label names list get labels list, where label name has been replaced by its identificator"
   ],
   "id": "e11ed3988dafd9e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:35.676967Z",
     "start_time": "2024-10-03T18:10:35.673515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_label_names(label_names: np.array) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Map the label names, each label name will have its own identificator.\n",
    "    Replace the label names with their unique identifier.\n",
    "    :param label_names: list of label names\n",
    "    :return: an array of labels, where now the label names have been replaced by their unique identifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    mapped_labels = {}\n",
    "    \n",
    "    # map the unique label names\n",
    "    for label, unique_label_name in enumerate(np.unique(label_names)):\n",
    "        mapped_labels[unique_label_name] = label\n",
    "\n",
    "    # replace label name by its identificator\n",
    "    labels = np.array([mapped_labels[label_name] for label_name in label_names])\n",
    "    \n",
    "    return labels"
   ],
   "id": "877316d53a12fe4e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:37.197748Z",
     "start_time": "2024-10-03T18:10:37.182043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels = map_label_names(train_label_names)\n",
    "test_labels = map_label_names(test_label_names)\n",
    "\n",
    "print(\"Train labels:\")\n",
    "print(train_labels.shape)\n",
    "print(train_labels, '\\n')\n",
    "\n",
    "print(\"Test labels:\")\n",
    "print(test_labels.shape)\n",
    "print(test_labels)"
   ],
   "id": "8fedd15c8f86ad28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:\n",
      "(28709,)\n",
      "[0 0 0 ... 6 6 6] \n",
      "\n",
      "Test labels:\n",
      "(7178,)\n",
      "[0 0 0 ... 6 6 6]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encode the labels into one hot vectors\n",
   "id": "c5e254108e6ab149"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:40.101210Z",
     "start_time": "2024-10-03T18:10:40.098080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(labels: np.array, num_classes: int):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode the labels 1D vector into one-hot vectors encoding.\n",
    "    One hot encoded vector has all zeros, but only one 1.\n",
    "    \n",
    "    :param labels: 1D vector of labels \n",
    "        F.e.:\n",
    "        happy: 0\n",
    "        sad: 1\n",
    "        angry: 2\n",
    "        labels: [0, 0, 1, 1, 2]\n",
    "        \n",
    "    :param num_classes: number of unique classes - of unique labels (f.e. 3 - happy, sad, and angry)\n",
    "    :return: a 2D one hot encoded array.\n",
    "    \n",
    "            F.e.:\n",
    "            labels = [0, 0, 1, 1, 2]\n",
    "            one_hot = \n",
    "                [\n",
    "                    [1 0 0]\n",
    "                    [1 0 0]\n",
    "                    [0 1 0]\n",
    "                    [0 1 0]\n",
    "                    [0 0 1]\n",
    "                ]\n",
    "            - shape of one_hot: (n_labels, unique_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # an array full of zeros of shape (num_labels, num_classes) \n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    \n",
    "    # set the 1 to appropriate labels\n",
    "    for n_row, label in enumerate(labels):\n",
    "        one_hot[n_row, label] = 1\n",
    "    \n",
    "    return one_hot"
   ],
   "id": "457f69a8205cc79c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:42.144044Z",
     "start_time": "2024-10-03T18:10:42.133330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_classes = len(np.unique(test_labels))\n",
    "\n",
    "train_labels_one_hot = one_hot_encode(train_labels, num_classes)\n",
    "test_labels_one_hot = one_hot_encode(test_labels, num_classes)\n",
    "\n",
    "print(\"Train labels one-hot:\")\n",
    "print(train_labels_one_hot.shape)\n",
    "print(train_labels_one_hot, '\\n')\n",
    "\n",
    "print(\"Test labels one-hot:\")\n",
    "print(test_labels_one_hot.shape)\n",
    "print(test_labels_one_hot, '\\n')"
   ],
   "id": "17ea33015821beae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels one-hot:\n",
      "(28709, 7)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] \n",
      "\n",
      "Test labels one-hot:\n",
      "(7178, 7)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final Representation of Data\n",
    "\n",
    "\n",
    "Training and testing sets:  \n",
    "- image arrays: train_images and test_images\n",
    "- one-hot encoded vector arrays: train_labels_one_hot, test_labels_one_hot"
   ],
   "id": "7453b4f228769d08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:44.637539Z",
     "start_time": "2024-10-03T18:10:44.635121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train Images (first):\")\n",
    "print(train_images.shape)\n",
    "print(train_images[0], '\\n')\n",
    "\n",
    "print(\"Test Images (first):\")\n",
    "print(test_images.shape)\n",
    "print(test_images[0], '\\n')\n",
    "\n",
    "print(\"Train Labels one-hot:\")\n",
    "print(train_labels_one_hot.shape)\n",
    "print(train_labels_one_hot, '\\n')\n",
    "\n",
    "print(\"Test Labels one-hot:\")\n",
    "print(test_labels_one_hot.shape)\n",
    "print(test_labels_one_hot, '\\n')"
   ],
   "id": "3e494c38a77f17dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images (first):\n",
      "(28709, 48, 48)\n",
      "[[0.70980392 0.70196078 0.69411765 ... 0.71372549 0.71372549 0.71372549]\n",
      " [0.70196078 0.69803922 0.69019608 ... 0.70196078 0.69411765 0.68627451]\n",
      " [0.70196078 0.69803922 0.69019608 ... 0.67843137 0.70196078 0.7254902 ]\n",
      " ...\n",
      " [0.76862745 0.70980392 0.74901961 ... 0.90196078 0.89411765 0.80392157]\n",
      " [0.76078431 0.72941176 0.78431373 ... 0.89019608 0.87058824 0.91372549]\n",
      " [0.77647059 0.77254902 0.83137255 ... 0.88627451 0.85882353 0.95294118]] \n",
      "\n",
      "Test Images (first):\n",
      "(7178, 48, 48)\n",
      "[[0.3254902  0.29411765 0.24313725 ... 0.12156863 0.12156863 0.12156863]\n",
      " [0.23921569 0.20784314 0.20784314 ... 0.12156863 0.12156863 0.1254902 ]\n",
      " [0.16862745 0.14901961 0.13333333 ... 0.14117647 0.11764706 0.10588235]\n",
      " ...\n",
      " [0.08235294 0.07843137 0.07843137 ... 0.03921569 0.04313725 0.0627451 ]\n",
      " [0.09411765 0.08627451 0.07843137 ... 0.06666667 0.04313725 0.02745098]\n",
      " [0.08627451 0.08235294 0.05882353 ... 0.04705882 0.03529412 0.03921569]] \n",
      "\n",
      "Train Labels one-hot:\n",
      "(28709, 7)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] \n",
      "\n",
      "Test Labels one-hot:\n",
      "(7178, 7)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convolutional Layer",
   "id": "722a9283be8dba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Custom Errors\n",
    "Made custom errors, which can be raised in the convolutional layer."
   ],
   "id": "15f83df2028defab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:10:49.385500Z",
     "start_time": "2024-10-03T18:10:49.380974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InvalidNumberOfFilters(Exception):\n",
    "    \"\"\" Invalid number of filters specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid number of filters specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "\n",
    "class InvalidFilterSize(Exception):\n",
    "    \"\"\" Invalid filter size specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid filter size specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "        \n",
    "class InvalidNumberOfChannels(Exception):\n",
    "    \"\"\" Invalid number of channels specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid number of channels specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "        \n",
    "class InvalidPadding(Exception):\n",
    "    \"\"\" Invalid padding specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid padding specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "\n",
    "class InvalidStride(Exception):\n",
    "    \"\"\" Invalid stride specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid stride specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ],
   "id": "7a273879b5674dcd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:40:56.249421Z",
     "start_time": "2024-10-03T19:40:56.238595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvLayer:\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters: int, filter_size: int, num_channels: int):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function constructor, initialize the filter array.\n",
    "        \n",
    "        :param num_filters: number of filters in the convolutional layer\n",
    "        :param filter_size: size of the filter, num_channels x filter_size x filter_size\n",
    "        :param num_channels: depth of the filter - kernel\n",
    "        :raises \n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(num_filters, int):\n",
    "            raise ValueError(\"Number of filters \\\"num_filters\\\" must be an integer!\")\n",
    "        elif num_filters < 1:\n",
    "            raise InvalidNumberOfFilters(f\"Number of filters \\\"num_filters\\\" must be at least 1, \\\"num_filters={num_filters}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(filter_size, int):\n",
    "            raise ValueError(\"Filter - kernel size \\\"filter_size\\\" must be an integer!\")\n",
    "        elif filter_size < 2:\n",
    "            raise InvalidFilterSize(f\"Filter - kernel size \\\"filter_size\\\" must be at least 2, \\\"filter_size={filter_size}\\\" specified instead!\")\n",
    "            \n",
    "        if not isinstance(num_channels, int):\n",
    "            raise ValueError(\"Number of channels \\\"num_channels\\\" must be an integer!\")\n",
    "        elif num_channels < 1:\n",
    "            raise InvalidNumberOfChannels(f\"Number of channels \\\"num_channels\\\" must be at least 1, \\\"num_channels={num_channels}\\\" was specified instead!\")\n",
    "            \n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # creates a 4D numpy array\n",
    "        # filter is of size (c x f x f), where f is the filter_size, and c is the num_channels\n",
    "        # (c x f x f) because image size is expected to be (c x height x width)\n",
    "        self.filters = np.random.randn(num_filters, num_channels, filter_size, filter_size) / filter_size ** 2\n",
    "    \n",
    "    \n",
    "    def shift_filter_window(self, image: np.array, stride: int) -> (np.array, int, int):\n",
    "        \n",
    "        \"\"\"\n",
    "        Shift the filter window to get all the regions in the image.\n",
    "        These regions are for the convolutional operations with the kernel - filter.\n",
    "        \n",
    "        :param image: image from which to get all the regions from.\n",
    "        :param stride: size of the stride, shift size - how big a step/shift\n",
    "        :return: a region (where input image and kernel intersect) which is a 2D array, and (i, j) coordinates of the region  \n",
    "        \"\"\"\n",
    "        \n",
    "        _, height, width = image.shape\n",
    "        \n",
    "        # shift the filter window, to process all regions in the input image\n",
    "        for i in range(0, height - self.filter_size + 1, stride):\n",
    "            for j in range(0, width - self.filter_size + 1, stride):\n",
    "                # extract the region of the image where the filter is applied\n",
    "                region = image[:, i: (i + self.filter_size), j: (j + self.filter_size)]\n",
    "                yield region, i, j  # what yield does that it returns a value, but this function has to be interated to get next() values\n",
    "                \n",
    "                \n",
    "    def forward(self, image: np.array, stride: int = 1, padding: int = 0) -> np.array:\n",
    "\n",
    "        \"\"\"\n",
    "        Perform forward pass through the convolutional layer.\n",
    "        Perform convolutional operations with the kernel - filter \n",
    "        \n",
    "        :param image: image to perform convolutional operation on \n",
    "        :param padding: padding applied to the image\n",
    "        :param stride: stride applied to the convolutional operation\n",
    "        :return: a convoluted image\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(stride, int):\n",
    "            raise ValueError(\"Stride must be an integer!\")\n",
    "        elif stride < 1:\n",
    "            raise InvalidStride(f\"Stride must be at least 1, \\\"stride={stride}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(padding, int):\n",
    "            raise ValueError(\"Padding must be an integer!\")\n",
    "        elif padding < 0:\n",
    "            raise InvalidPadding(f\"Padding must be at least 0, \\\"padding={padding}\\\" specified instead!\")\n",
    "        \n",
    "        # if image is 2D (single channel, number of channels is 1), expand from (height x width) into (1 x height x width) so it has a depth=1\n",
    "        if image.ndim == 2:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "        padded_image = self.pad_image(image, padding)  # apply padding to the image\n",
    "        _, height, width = image.shape  # get only height and width, image is of size (n_channels x height x width)\n",
    "    \n",
    "         # check if filter can fit into the image with padding\n",
    "        if height + 2 * padding < self.filter_size or width + 2 * padding < self.filter_size:\n",
    "            raise ValueError(\"Filter size is too large for the input image size!\")\n",
    "        \n",
    "        # expected output height and width\n",
    "        output_height = (height + 2 * padding - self.filter_size) // stride + 1\n",
    "        output_width = (width + 2 * padding - self.filter_size) // stride + 1\n",
    "        \n",
    "        output = np.zeros((self.num_filters, output_height, output_width))\n",
    "\n",
    "        for region, i, j in self.shift_filter_window(padded_image, stride):\n",
    "            for n_filter in range(self.num_filters):\n",
    "                # perform convolution on the region using the filter\n",
    "                output[n_filter, i // stride, j // stride] = self.conv(region, self.filters[n_filter]) \n",
    "        \n",
    "        return output\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def conv(image_region: np.array, kernel: np.array) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform convolutional operation on the image region using the filter.\n",
    "        Convolution operation is sum(image_region * filter)\n",
    "        \n",
    "        :param image_region: region to perform convolutional operation on\n",
    "        :param kernel: filter - kernel to perform convolutional operation with\n",
    "        :return: convolution result\n",
    "        \"\"\"\n",
    "        \n",
    "        result = 0\n",
    "  \n",
    "        # perform convolution for each channel separately and sum the result\n",
    "        for channel in range(image_region.shape[0]):  # loop over the channels\n",
    "            conv_result = convolve2d(image_region[channel], kernel[channel], mode=\"valid\")\n",
    "            result += conv_result.item()  # extract the scalar value if it is a 1 x 1 array\n",
    "\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def pad_image(image: np.array, padding: int) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Apply zero-padding to the input image with multiple channels.\n",
    "        \n",
    "        :param image: image to apply padding on\n",
    "        :param padding: padding applied to the image\n",
    "        :return: padded image\n",
    "        \"\"\"\n",
    "        \n",
    "        if padding > 0:\n",
    "            # apply padding to the height and width dimensions (1 and 2), keeping channels dimension intact\n",
    "            padded_image = np.pad(image, ((0, 0), (padding, padding), (padding, padding)), mode=\"constant\")\n",
    "        else:\n",
    "            padded_image = image\n",
    "            \n",
    "        return padded_image"
   ],
   "id": "7e0f8dfc5486b5e2",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pooling Layer - Max & Average Pooling Layer",
   "id": "7ea7b7eba9ee73fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Custom Errors\n",
    "Made custom errors, which can be raised in the pooling layer."
   ],
   "id": "1b7bba871e2be989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:40:57.751740Z",
     "start_time": "2024-10-03T19:40:57.748316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InvalidPoolSize(Exception):\n",
    "    \"\"\" Invalid pool size specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid pool size specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "        \n",
    "class InvalidPoolType(Exception):\n",
    "    \"\"\" Invalid pool type specified \"\"\"\n",
    "    def __init__(self, message=\"Invalid pool type specified by the user\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ],
   "id": "11412040dcf1e76d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:45:08.279357Z",
     "start_time": "2024-10-03T19:45:08.272223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PoolLayer:\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    POOL_TYPES = [\"max\", \"avg\"]\n",
    "    \n",
    "    def __init__(self, pool_size: int, stride: int, pool_type: str):\n",
    "        \n",
    "        \"\"\"\n",
    "        Function constructor, initialize the pool size and stride.\n",
    "        \n",
    "        :param pool_size: height and width of the pool for max pooling layer\n",
    "        :param stride: stride size (how large s shift in the input image)\n",
    "        :param pool_type: type of pooling layer, it is \"max\" or \"avg\" (avg stands for average)\n",
    "        :raises \n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(pool_size, int):\n",
    "            raise ValueError(\"Pool size \\\"pool_size\\\" must be an integer!\")\n",
    "        elif pool_size < 1:\n",
    "            raise InvalidPoolSize(f\"Pool size \\\"pool_size\\\" must be at least 1, \\\"pool_size={pool_size}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(stride, int):\n",
    "            raise ValueError(\"Stride must be an integer!\")\n",
    "        elif stride < 1:\n",
    "            raise InvalidStride(f\"Stride must be at least 1, \\\"stride={stride}\\\" specified instead!\")\n",
    "        \n",
    "        if not isinstance(pool_type, str):\n",
    "            raise ValueError(f\"Pool type must be a string, available pool types: {', '.join(self.POOL_TYPES)}\")\n",
    "        elif pool_type.lower() not in self.POOL_TYPES:\n",
    "            raise InvalidPoolType(f\"Pool type must be either {', '.join(self.POOL_TYPES)}, \\\"pool_type={pool_type}\\\" specified instead!\")\n",
    "        \n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.pool_type = pool_type.lower()\n",
    "    \n",
    "    def forward(self, image: np.array) -> np.array:\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform max/average pooling operation on the input image.\n",
    "        \n",
    "        :param image: input image (size channels x height x width).\n",
    "        :return: downsampled output image\n",
    "        \"\"\"\n",
    "        \n",
    "        # if image is 2D (single channel, number of channels is 1), expand from (height x width) into (1 x height x width) so it has a depth=1\n",
    "        if image.ndim == 2:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            \n",
    "        num_channels, height, width = image.shape\n",
    "        \n",
    "        # calculate output dimensions\n",
    "        output_height = (height - self.pool_size) // self.stride + 1\n",
    "        output_width = (width - self.pool_size) // self.stride + 1\n",
    "        \n",
    "        # initialize the output array\n",
    "        pooled_output = np.zeros((num_channels, output_height, output_width))\n",
    "        \n",
    "        # perform max or average pooling depending on the pool type\n",
    "        if self.pool_type == \"max\":\n",
    "            pooling_function = np.max\n",
    "        else:\n",
    "            pooling_function = np.mean\n",
    "            \n",
    "        # perform max/average pooling\n",
    "        for channel in range(num_channels):\n",
    "            for region, i, j in self.shift_pool_window(image, channel):\n",
    "                pooled_output[channel, i // self.stride, j // self.stride] = pooling_function(region)\n",
    "\n",
    "        return pooled_output  # return a downsampled output image \n",
    "    \n",
    "    \n",
    "    def shift_pool_window(self, image: np.array, channel: int) -> (np.array, int, int):\n",
    "        \n",
    "        \"\"\"\n",
    "        Shift the pool window to get all the regions in the image.\n",
    "        These regions are for the max/average pooling operation\n",
    "        \n",
    "        :param image: image from which to get all the regions from.\n",
    "        :param channel: channel index\n",
    "        :return: a region (where input image and pool intersect) which is a 2D array, and (i, j) coordinates of the region  \n",
    "        \"\"\"\n",
    "        \n",
    "        _, height, width = image.shape\n",
    "        \n",
    "        # shift the pool window, to process all regions in the input image\n",
    "        for i in range(0, height - self.pool_size + 1, self.stride):\n",
    "            for j in range(0, width - self.pool_size + 1, self.stride):\n",
    "                # extract the region of the image where the pool is\n",
    "                region = image[channel, i:i + self.pool_size, j:j + self.pool_size]\n",
    "                yield region, i, j  # what yield does that it returns a value, but this function has to be interated to get next() values            "
   ],
   "id": "2b6321a746e59830",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:54:44.942555Z",
     "start_time": "2024-10-03T19:48:25.206847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example of using one convolutional layer, using only forward pass\n",
    "\n",
    "conv_layer1 = ConvLayer(num_filters=3, filter_size=4, num_channels=1)\n",
    "max_layer1 = PoolLayer(pool_size=2, stride=2, pool_type=\"max\")\n",
    "\n",
    "conv_layer2 = ConvLayer(num_filters=1, filter_size=2, num_channels=3)\n",
    "avg_layer2 = PoolLayer(pool_size=3, stride=2, pool_type=\"avg\")\n",
    "\n",
    "batch_size = 300\n",
    "convolved_images = []\n",
    "\n",
    "for i in range(0, train_images.shape[0], batch_size):  # loop over all the number of images (28709)\n",
    "    \n",
    "    print(f\"{(i // batch_size) + 1}. batch {i}/{train_images.shape[0]}\")\n",
    "    \n",
    "    convolved_batch = []\n",
    "    batch_images = train_images[i: i + batch_size]  # Get the i-th image (shape 48x48)\n",
    "\n",
    "    for image in batch_images:\n",
    "        convolved_image = conv_layer1.forward(image)\n",
    "        max_pooled_image = max_layer1.forward(convolved_image)\n",
    "        convolved_image2 = conv_layer2.forward(max_pooled_image)\n",
    "        avg_pooled_image = avg_layer2.forward(convolved_image2)\n",
    "        \n",
    "        convolved_batch.append(avg_pooled_image)\n",
    "    \n",
    "    convolved_images.extend(np.array(convolved_batch))\n",
    "\n",
    "convolved_images = np.array(convolved_images)"
   ],
   "id": "ef5198811a4a8416",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. batch 0/28709\n",
      "2. batch 300/28709\n",
      "3. batch 600/28709\n",
      "4. batch 900/28709\n",
      "5. batch 1200/28709\n",
      "6. batch 1500/28709\n",
      "7. batch 1800/28709\n",
      "8. batch 2100/28709\n",
      "9. batch 2400/28709\n",
      "10. batch 2700/28709\n",
      "11. batch 3000/28709\n",
      "12. batch 3300/28709\n",
      "13. batch 3600/28709\n",
      "14. batch 3900/28709\n",
      "15. batch 4200/28709\n",
      "16. batch 4500/28709\n",
      "17. batch 4800/28709\n",
      "18. batch 5100/28709\n",
      "19. batch 5400/28709\n",
      "20. batch 5700/28709\n",
      "21. batch 6000/28709\n",
      "22. batch 6300/28709\n",
      "23. batch 6600/28709\n",
      "24. batch 6900/28709\n",
      "25. batch 7200/28709\n",
      "26. batch 7500/28709\n",
      "27. batch 7800/28709\n",
      "28. batch 8100/28709\n",
      "29. batch 8400/28709\n",
      "30. batch 8700/28709\n",
      "31. batch 9000/28709\n",
      "32. batch 9300/28709\n",
      "33. batch 9600/28709\n",
      "34. batch 9900/28709\n",
      "35. batch 10200/28709\n",
      "36. batch 10500/28709\n",
      "37. batch 10800/28709\n",
      "38. batch 11100/28709\n",
      "39. batch 11400/28709\n",
      "40. batch 11700/28709\n",
      "41. batch 12000/28709\n",
      "42. batch 12300/28709\n",
      "43. batch 12600/28709\n",
      "44. batch 12900/28709\n",
      "45. batch 13200/28709\n",
      "46. batch 13500/28709\n",
      "47. batch 13800/28709\n",
      "48. batch 14100/28709\n",
      "49. batch 14400/28709\n",
      "50. batch 14700/28709\n",
      "51. batch 15000/28709\n",
      "52. batch 15300/28709\n",
      "53. batch 15600/28709\n",
      "54. batch 15900/28709\n",
      "55. batch 16200/28709\n",
      "56. batch 16500/28709\n",
      "57. batch 16800/28709\n",
      "58. batch 17100/28709\n",
      "59. batch 17400/28709\n",
      "60. batch 17700/28709\n",
      "61. batch 18000/28709\n",
      "62. batch 18300/28709\n",
      "63. batch 18600/28709\n",
      "64. batch 18900/28709\n",
      "65. batch 19200/28709\n",
      "66. batch 19500/28709\n",
      "67. batch 19800/28709\n",
      "68. batch 20100/28709\n",
      "69. batch 20400/28709\n",
      "70. batch 20700/28709\n",
      "71. batch 21000/28709\n",
      "72. batch 21300/28709\n",
      "73. batch 21600/28709\n",
      "74. batch 21900/28709\n",
      "75. batch 22200/28709\n",
      "76. batch 22500/28709\n",
      "77. batch 22800/28709\n",
      "78. batch 23100/28709\n",
      "79. batch 23400/28709\n",
      "80. batch 23700/28709\n",
      "81. batch 24000/28709\n",
      "82. batch 24300/28709\n",
      "83. batch 24600/28709\n",
      "84. batch 24900/28709\n",
      "85. batch 25200/28709\n",
      "86. batch 25500/28709\n",
      "87. batch 25800/28709\n",
      "88. batch 26100/28709\n",
      "89. batch 26400/28709\n",
      "90. batch 26700/28709\n",
      "91. batch 27000/28709\n",
      "92. batch 27300/28709\n",
      "93. batch 27600/28709\n",
      "94. batch 27900/28709\n",
      "95. batch 28200/28709\n",
      "96. batch 28500/28709\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T10:12:49.980916Z",
     "start_time": "2024-10-02T10:12:49.977969Z"
    }
   },
   "cell_type": "code",
   "source": "convolved_images.shape",
   "id": "cce03ea6e23eeaeb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 3, 45, 45)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c5b28d6583bc3714"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
